{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sincere_clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Importing Libraries and defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install scikit-fuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Fuzzy model imports\n",
    "\n",
    "import skfuzzy as fuzz\n",
    "from fcmeans import FCM\n",
    "\n",
    "#Data Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "#Sci-kit learn imports\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score\n",
    "\n",
    "# Blocking warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Method for K means using Yellowbrick lib\n",
    "def Yelbow(df):\n",
    "    model = KMeans()\n",
    "    visualizer = KElbowVisualizer(model, k=(2,21), timings= True, random_state=8) # k is range of number of clusters.\n",
    "    visualizer.fit(df)        # Fit data to visualizer\n",
    "    visualizer.show()        # Finalize and render figure\n",
    "\n",
    "# Silhouette for kmeans using Yellowbrick lib\n",
    "def Ysilhouette(df):\n",
    "    model = KMeans()\n",
    "    visualizer = KElbowVisualizer(model, k=(2,21),metric='silhouette', timings= True, random_state=8) # k is range of number of clusters.\n",
    "    visualizer.fit(df)        # Fit the data to the visualizer\n",
    "    visualizer.show()        # Finalize and render the figure\n",
    "\n",
    "#Davies Bouldin from sklearn\n",
    "def DaviesBouldin(data, center):\n",
    "\n",
    "    #instantiate kmeans\n",
    "    kmeans = KMeans(n_clusters=center)\n",
    "    # Then fit the model to your data using the fit method\n",
    "    model = kmeans.fit_predict(data)\n",
    "    \n",
    "    # Calculate Davies Bouldin score\n",
    "    score = davies_bouldin_score(data, model)\n",
    "    \n",
    "    return score\n",
    "\n",
    "#Dendrogram viz\n",
    "def VizDendrogram(df):\n",
    "    plt.figure(figsize=(10, 7))  \n",
    "    plt.title(\"Dendrograms df_merged_kmeans_sample\")  \n",
    "    dend = shc.dendrogram(shc.linkage(df, method='ward'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Performing itens clustering using K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data\n",
    "df_movies_4kmeans = pd.read_csv('df_movies_4kmeans.csv')\n",
    "\n",
    "# dropping unecessary columns\n",
    "df_movies_4kmeans_filtered = df_movies_4kmeans.drop(['movieId','age_movie_when_rated','time_of_day_1','time_of_day_2','time_of_day_3','time_of_day_4','weekday_0','weekday_1', 'weekday_2', 'weekday_3','weekday_4','weekday_5', 'weekday_6', 'title', 'imdbId', 'Year', 'imdbVotes'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Creating a sample to search for the optmal K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full database stats listed\n",
    "df_merged_kmeans_stats = df_movies_4kmeans_filtered.describe()\n",
    "df_merged_kmeans_stats.drop(['count', 'min','25%','75%','max'], inplace=True, axis=0)\n",
    "df_merged_kmeans_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting a sample of the database and listing stats\n",
    "df_merged_kmeans_sample = df_movies_4kmeans_filtered.sample(frac=0.30, random_state=8)\n",
    "df_merged_kmeans_sample_stats = df_merged_kmeans_sample.describe()\n",
    "df_merged_kmeans_sample_stats.drop(['count', 'min','25%','75%','max'], inplace=True, axis=0)\n",
    "df_merged_kmeans_sample_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Looking for the best k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajusting scale of the sample dataset using min max scaler\n",
    "df_merged_kmeans_sample = StandardScaler().fit_transform(df_merged_kmeans_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ysilhouette(df_merged_kmeans_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yelbow(df_merged_kmeans_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Davies Bouldin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "centers = list(range(2,30))\n",
    "\n",
    "for center in centers:\n",
    "    scores.append(DaviesBouldin(df_merged_kmeans_sample, center))\n",
    "    \n",
    "plt.plot(centers, scores, linestyle='--', marker='o', color='b');\n",
    "plt.xlabel('K');\n",
    "plt.ylabel('Davies Bouldin score');\n",
    "plt.title('Davies Bouldin score vs. K');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VizDendrogram(df_merged_kmeans_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding the selection of the number of clusters, based on the above metrics plots and the heuristic perception of the optimal ammount of clusters, K was set to 11 to group users in order to best adressing sincere results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Clusterizing items with full database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusting variable scale\n",
    "df_kmeans_cluster_full = StandardScaler().fit_transform(df_movies_4kmeans_filtered)\n",
    "\n",
    "# running Kmeans cosidering 11 centroids\n",
    "kmeans = KMeans(n_clusters=11, init='k-means++', n_init=100, max_iter=5000, random_state=8).fit(df_kmeans_cluster_full)\n",
    "\n",
    "# saving complete results with cluster labels to csv\n",
    "df_movies_4kmeans.insert(0, 'movie_cluster', kmeans.labels_)\n",
    "\n",
    "# saving clustering results only to csv\n",
    "df_movies_4kmeans.reset_index(drop=True)\n",
    "df_movies_4kmeans.to_csv('df_movies_clustered.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking results\n",
    "df_movies_4kmeans.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Generating input database for Fuzzy C-Means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing user wrangled data\n",
    "df_kmeans_results = df_movies_4kmeans[['movieId','movie_cluster']]\n",
    "df_user_ratings = pd.read_csv('df_ratings_by_time.csv')\n",
    "df_user_ratings = df_user_ratings[['userId', 'movieId', 'rating']]\n",
    "\n",
    "# merging data to create preliminar df_sincere  dataset\n",
    "df_sincere = df_user_ratings.merge(df_kmeans_results, on='movieId', how='left')\n",
    "df_sincere.to_csv('df_sincere_database.csv', index=False)\n",
    "\n",
    "# generate input for fuzzy c-means\n",
    "df_sincere_grouped = pd.get_dummies(data=df_sincere, columns=['movie_cluster'])\n",
    "df_sincere_grouped = df_sincere_grouped.drop('movieId', axis=1)\n",
    "df_sincere_grouped = df_sincere_grouped.groupby('userId').sum().reset_index()\n",
    "df_fuzzy_cmeans_input = df_sincere_grouped.drop(['userId','rating'], axis=1)\n",
    "\n",
    "# storing fuzzy c-means input in a csv file \n",
    "df_fuzzy_cmeans_input.to_csv('df_users_4clustering.csv', index=False)\n",
    "\n",
    "# checking results\n",
    "df_fuzzy_cmeans_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Clusterizing users using fuzzy c-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shortcut | reading from stored intermediate results\n",
    "#df_fuzzy_cmeans_input = pd.read_csv('df_users_4clustering.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using standard scaler to normalize data\n",
    "df_fuzzy_cmeans = StandardScaler().fit_transform(df_fuzzy_cmeans_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the best k\n",
    "for i in range(2,10):\n",
    "    fuzzycmeans = fuzz.cmeans(df_fuzzy_cmeans.T, c=i, m=1.7, error = 000.5, maxiter=5000, init=None, seed=1)\n",
    "    cntr, u, u0, d, jm, p, fpc = fuzzycmeans\n",
    "    print(i,'clusters fpc score:',fpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model with k=8\n",
    "fuzzycmeans = fuzz.cmeans(df_fuzzy_cmeans.T, c=8, m=1.7, error = 000.5, maxiter=5000, init=None, seed=1)\n",
    "cntr, u, u0, d, jm, p, fpc = fuzzycmeans\n",
    "\n",
    "# transposing fuzzy into a matrix results\n",
    "df_cmeans_full = pd.DataFrame(u.T)\n",
    "df_cmeans_full.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting softclustering to hard clustering by labeling users by its user clusrter with higher fuzzy c-means index\n",
    "cmeanslabels = np.argmax(u,axis=0)\n",
    "\n",
    "# inserting fuzzy labels on df_cmeans\n",
    "df_fuzzy_cmeans_output = df_sincere_grouped\n",
    "df_fuzzy_cmeans_output.insert(0, 'user_cluster', cmeanslabels)\n",
    "df_fuzzy_cmeans_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing results into csv files\n",
    "df_fuzzy_cmeans_output.to_csv('df_users_clustered.csv', index=False)\n",
    "df_cmeans_full.to_csv('df_users_fuzzyCmeans_matrix_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D) Sincere database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shortcut | reading from stored intermediate results\n",
    "#df_sincere = pd.read_csv('df_sincere_database.csv')\n",
    "#df_fuzzy_cmeans_output = pd.read_csv('df_users_clustered.csv')\n",
    "\n",
    "# updading sincere database to csv\n",
    "df_fuzzy_results = df_fuzzy_cmeans_output[['userId', 'user_cluster']]\n",
    "df_sincere = df_sincere.merge(df_fuzzy_results, on='userId', how='inner').dropna()\n",
    "df_sincere.to_csv('df_sincere_database.csv', index=False)\n",
    "\n",
    "# checking results\n",
    "df_sincere"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
